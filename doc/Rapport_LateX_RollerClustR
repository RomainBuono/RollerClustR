\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{a4paper, margin=2.5cm}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{hyperref}

% En-têtes et pieds de page
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Méthodologie - RollerClustR}
\fancyhead[R]{\small B. Buono}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

\onehalfspacing

\title{\textbf{Méthodologie de développement du package RollerClustR}\\
\Large }
\author{Romain BUONO, Nico DENA, Mohamed Habib BAH\\
\small Master 2 SISE, Université Lyon 2}
\date{Novembre 2025}

\begin{document}

\maketitle

\begin{abstract}
Ce rapport présente la méthodologie de développement du package R \texttt{RollerClustR}, une bibliothèque dédiée au clustering de variables implémentée selon le paradigme de programmation orientée objet R6. Le package intègre trois algorithmes complémentaires : VAR\_CAH (Classification Ascendante Hiérarchique de variables), VAR\_KMEANS (K-Means pour variables) et TandemVarClust (clustering de modalités pour données mixtes). L'architecture logicielle repose sur les principes SOLID et le pattern Factory, garantissant extensibilité, maintenabilité et cohérence d'interface. Une documentation technique exhaustive pour chaque algorithme est disponible dans le répertoire \texttt{/doc} du package.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction et Contexte}

\subsection{Problématique du Clustering de Variables}

Le clustering de variables constitue une approche complémentaire au clustering d'observations traditionnel. Plutôt que de regrouper des individus similaires, l'objectif consiste à identifier des ensembles de variables présentant des structures de dépendance similaires. Cette démarche trouve des applications majeures en :

\begin{itemize}
    \item \textbf{Réduction de dimensionnalité} : Synthèse de $p$ variables corrélées en $k$ composantes latentes ($k \ll p$)
    \item \textbf{Feature engineering} : Construction de variables synthétiques non redondantes
    \item \textbf{Détection de multicolinéarité} : Identification de groupes de variables explicatives redondantes
    \item \textbf{Analyse exploratoire} : Compréhension de la structure de corrélation dans des jeux de données de haute dimension
\end{itemize}

Dans des contextes de haute dimensionnalité (génomique, chimiométrie, analyse sensorielle), où le nombre de variables $p$ peut excéder plusieurs milliers, les approches de clustering de variables offrent une alternative computationnellement efficace aux méthodes de régularisation classiques.

\subsection{Choix du Paradigme R6}

Le développement de \texttt{RollerClustR} repose sur la classe R6, un système de programmation orientée objet pour R offrant :

\begin{enumerate}
    \item \textbf{Encapsulation} : Distinction claire entre méthodes publiques (API) et attributs privés (implémentation)
    \item \textbf{Héritage} : Factorisation du code via une classe abstraite \texttt{ClusterAnalysis}
    \item \textbf{Persistance} : Stockage d'état interne permettant la réutilisation de modèles ajustés
    \item \textbf{Performance} : Modification en place évitant les copies mémoire coûteuses
\end{enumerate}

Contrairement aux systèmes S3 et S4 natifs de R, R6 implémente une approche par référence similaire aux langages orientés objet classiques (Java, Python), facilitant la conception d'architectures complexes.

\subsection{Objectifs du Package}

Le package \texttt{RollerClustR} vise à fournir :

\begin{itemize}
    \item Une \textbf{interface unifiée} pour trois algorithmes de clustering distincts
    \item Des \textbf{fonctionnalités de prédiction} permettant l'assignation incrémentale de nouvelles variables
    \item Une \textbf{architecture extensible} facilitant l'intégration de futurs algorithmes
    \item Une \textbf{documentation technique détaillée} dans \texttt{/doc} pour chaque algorithme
    \item Une \textbf{interface Shiny} pour l'exploration interactive
\end{itemize}

\section{Architecture Logicielle Globale}

\subsection{Hiérarchie de Classes}

L'architecture de \texttt{RollerClustR} s'articule autour d'une classe abstraite \texttt{ClusterAnalysis} dont héritent les trois implémentations algorithmiques. Cette hiérarchie garantit une interface homogène via les méthodes abstraites : \texttt{fit(X)}, \texttt{predict(newdata)}, et \texttt{summary()}.

\subsection{Pattern Factory}

Le module \texttt{ClusteringFactory} implémente le pattern de conception Factory, encapsulant la logique d'instanciation des algorithmes. Cette approche offre plusieurs avantages architecturaux :

\begin{enumerate}
    \item \textbf{Découplage} : Le code client n'a pas besoin de connaître les détails d'instanciation
    \item \textbf{Cohérence} : Paramétrage uniforme malgré les différences d'implémentation
    \item \textbf{Extensibilité} : Ajout de nouveaux algorithmes sans modification du code existant
\end{enumerate}

\subsection{Documentation Technique}

Le répertoire \texttt{/doc} contient des notices méthodologiques complètes pour chaque algorithme, incluant les fondements théoriques, les détails d'implémentation, les formules mathématiques et les cas d'usage. Ces documents constituent une référence exhaustive pour comprendre le comportement précis de chaque méthode.

\section{VAR\_CAH : Classification Hiérarchique de Variables}

\subsection{Fondements Théoriques}

L'algorithme VAR\_CAH réalise un clustering hiérarchique ascendant (agglomératif) de variables quantitatives. Le processus repose sur trois étapes fondamentales :

\subsubsection{Construction de la Matrice de Dissimilarité}

La dissimilarité entre variables est définie par la transformation de la corrélation de Pearson :

\begin{equation}
d(X_i, X_j) = 1 - |\rho(X_i, X_j)|
\end{equation}

où $\rho(X_i, X_j)$ désigne le coefficient de corrélation entre les variables $X_i$ et $X_j$. L'utilisation de la valeur absolue garantit que des variables fortement corrélées négativement sont considérées comme similaires.

\subsubsection{Algorithme Hiérarchique avec Critère de Ward}

La classification ascendante est réalisée selon le critère de Ward, qui minimise l'inertie intra-classe à chaque étape de fusion. On agrège les clusters $C_i$ et $C_j$ qui minimisent :

\begin{equation}
\Delta(C_i, C_j) = \frac{n_i n_j}{n_i + n_j} \|m_i - m_j\|^2
\end{equation}

où $n_i$ et $n_j$ sont les effectifs (nombre de variables) des clusters.

\subsubsection{Synthèse par Composante Principale}

Pour chaque cluster $C_k$ obtenu, une Analyse en Composantes Principales est effectuée. La première composante principale est retenue comme variable synthétique $S_k$ :

\begin{equation}
S_k = \sum_{j \in C_k} w_j X_j
\end{equation}

où les poids $w_j$ correspondent au premier vecteur propre de la matrice de corrélation.

\subsection{Innovations Méthodologiques}

\subsubsection{Reconfiguration Dynamique}

L'implémentation propose une méthode \texttt{refit\_with\_k(new\_k)} permettant de modifier le nombre de clusters sans réexécuter l'algorithme complet. Le dendrogramme est simplement coupé à un nouveau niveau, puis les variables synthétiques sont recalculées.

\subsubsection{Méthodes de Prédiction Duales}

Deux stratégies d'assignation de nouvelles variables sont implémentées : la méthode "correlation" calcule la corrélation avec chaque variable synthétique $S_k$ (complexité $O(Kn)$), tandis que la méthode "distance" calcule la corrélation moyenne avec toutes les variables du cluster.

\subsection{Complexité Algorithmique}

La complexité globale est $O(p^2 n)$, rendant la méthode adaptée pour $p \leq 10^3$ variables.

\section{VAR\_KMEANS : K-Means pour Variables}

\subsection{Principe Général}

L'algorithme VAR\_KMEANS adapte le paradigme K-Means classique au contexte du clustering de variables, selon les travaux de Vigneau et Qannari (2003). Contrairement au K-Means standard, l'objet classé concerne des variables (colonnes) basées sur leurs profils, et le centre d'un cluster est la première composante principale du groupe.

\subsection{Architecture Itérative}

L'algorithme alterne entre deux étapes jusqu'à convergence : l'étape d'allocation affecte chaque variable $x_j$ au cluster $k$ maximisant le coefficient de corrélation au carré :

\begin{equation}
k^* = \arg\max_{k \in \{1,\ldots,K\}} r^2(x_j, u_k)
\end{equation}

où $u_k$ est le centre (composante principale) du cluster $k$. L'étape de représentation recalcule pour chaque cluster $C_k$ le nouveau centre $u_k$ comme la première composante principale des variables du cluster.

\subsection{Stratégie Multi-Initialisation}

Le K-Means étant sensible aux optimums locaux, l'algorithme implémente une stratégie de redémarrage multiple (\texttt{n\_init}). Le critère d'optimisation est l'inertie totale :

\begin{equation}
T = \sum_{k=1}^K \sum_{j \in C_k} r^2(x_j, u_k)
\end{equation}

\subsection{Avantages Computationnels}

Contrairement à VAR\_CAH qui nécessite le calcul d'une matrice de dissimilarité $p \times p$, VAR\_KMEANS présente une complexité linéaire en nombre de variables : $O(I \cdot n \cdot p \cdot K)$ où $I$ est le nombre d'itérations (typiquement $< 100$). Cette propriété rend l'algorithme particulièrement adapté aux jeux de données de très haute dimension ($p > 10^4$).

\section{TandemVarClust : Clustering de Modalités}

\subsection{Distinction Fondamentale}

TandemVarClust se distingue des deux approches précédentes : il opère au niveau des \textbf{modalités} plutôt que des variables. Chaque catégorie d'une variable qualitative ou chaque intervalle discrétisé d'une variable quantitative constitue une unité à classer. Cette granularité fine permet de capturer des structures que le clustering de variables masquerait.

\subsection{Architecture Tandem Séquentielle}

L'approche combine deux étapes indépendantes : (1) l'AFDM (Analyse Factorielle de Données Mixtes) pour la réduction de dimensionnalité du tableau disjonctif, et (2) la CAH pour la classification hiérarchique sur les coordonnées factorielles des modalités. Cette approche séquentielle offre l'avantage de la simplicité algorithmique et de l'interprétabilité : les axes factoriels peuvent être examinés indépendamment.

\subsection{Prétraitement des Données Mixtes}

Pour les variables qualitatives, le codage disjonctif complet (indicatrices) est appliqué. Pour une variable $V$ à $m$ modalités, on crée $m$ colonnes binaires où $Z_{ij} = 1$ si l'observation $i$ possède la modalité $j$. Les variables quantitatives sont discrétisées en \texttt{n\_bins} intervalles (défaut : 5) par quantiles, puis codées par indicatrices. Le choix des quantiles garantit des effectifs équilibrés.

La matrice disjonctive finale $\mathbf{Z}$ a pour dimension :

\begin{equation}
n \times p^* \quad \text{où} \quad p^* = \sum_{j \in \text{qual}} m_j + \sum_{j \in \text{quant}} n_{\text{bins},j}
\end{equation}

\subsection{Analyse Factorielle : AFDM}

L'AFDM réalise une analyse factorielle sur $\mathbf{Z}$, combinant les principes de l'ACP et de l'ACM. Les modalités sont pondérées par l'inverse de leur fréquence marginale :

\begin{equation}
w_j = \frac{1}{f_j} \quad \text{où} \quad f_j = \frac{1}{n}\sum_{i=1}^n Z_{ij}
\end{equation}

Cette pondération compense le déséquilibre d'effectifs entre modalités. La décomposition en valeurs singulières (SVD) de la matrice pondérée fournit les coordonnées factorielles des modalités, qui sont ensuite utilisées pour le clustering hiérarchique.

\subsection{Méthode \texttt{predict\_variable()} : Projection AFDM}

Une innovation majeure de l'implémentation concerne la méthode de prédiction pour variables illustratives. Contrairement aux approches classiques, la nouvelle implémentation utilise une **projection dans l'espace AFDM** suivie d'un calcul de distances aux centres de clusters.

\subsubsection{Principe de la Projection}

Pour une nouvelle variable illustrative, les étapes sont :

\begin{enumerate}
    \item Transformation en tableau disjonctif (codage indicateur pour catégorielles, discrétisation puis codage pour numériques)
    \item Projection des modalités dans l'espace factoriel établi lors de l'entraînement :
    \begin{equation}
    \mathbf{f}_{\text{new}} = (\mathbf{z}_{\text{new}} - \bar{\mathbf{z}}) \mathbf{D}_{\text{col}}^{1/2} \mathbf{V}
    \end{equation}
    où $\mathbf{V}$ est la matrice des vecteurs singuliers à droite de la SVD d'entraînement
    \item Calcul des distances euclidiennes moyennes entre les modalités de la variable et les centres de chaque cluster
    \item Assignation au cluster de distance minimale
\end{enumerate}

\subsubsection{Métriques d'Association Complémentaires}

Au-delà de l'assignation basée sur les distances, la méthode calcule des métriques statistiques enrichies :

\begin{itemize}
    \item \textbf{Table de contingence} : Croisement de la variable illustrative avec les clusters d'observations
    \item \textbf{Test du Chi²} : Évaluation de l'indépendance entre variable et clusters
    \item \textbf{V de Cramér} : Mesure d'association normalisée entre 0 et 1 :
    \begin{equation}
    V = \sqrt{\frac{\chi^2}{n \times \min(K-1, m-1)}}
    \end{equation}
    \item \textbf{Scores de Dice} : Matrice donnant pour chaque observation son indice de Dice avec chaque cluster
\end{itemize}

Cette approche garantit la cohérence méthodologique avec la structure factorielle établie lors du clustering et fournit une analyse multidimensionnelle de la relation entre variable illustrative et partition.

\subsection{Assignation des Observations : Coefficient de Dice}

Pour assigner les observations aux clusters de modalités, l'indice de similarité de Dice est utilisé :

\begin{equation}
\text{Dice}(O, C_k) = \frac{2|O \cap C_k|}{|O| + |C_k|}
\end{equation}

où $O$ est l'ensemble des modalités actives pour l'observation et $C_k$ l'ensemble des modalités du cluster $k$. Le coefficient de Dice présente plusieurs avantages : traitement équilibré des tailles, interprétation naturelle comme proportion de chevauchement, et robustesse aux déséquilibres d'effectifs.

\section{Interface Shiny : Exploration Interactive}

\subsection{Architecture de l'Application}

L'application Shiny développée pour \texttt{RollerClustR} offre une interface utilisateur complète permettant l'exploration interactive des algorithmes de clustering de variables. L'architecture suit le modèle MVC (Model-View-Controller) avec trois composantes principales :

\begin{itemize}
    \item \textbf{global.R} : Chargement des packages, des classes R6, et définition des fonctions utilitaires
    \item \textbf{ui.R} : Interface utilisateur construite avec \texttt{shinydashboard}
    \item \textbf{server.R} : Logique serveur gérant les valeurs réactives et les interactions
\end{itemize}

\subsection{Fonctionnalités Principales}

\subsubsection{Gestion des Données}

L'onglet "Données" permet :
\begin{itemize}
    \item \textbf{Import de fichiers} : Support des formats CSV (virgule, point-virgule, tabulation) et Excel (.xlsx, .xls)
    \item \textbf{Datasets pré-chargés} : Accès direct à 6 datasets R classiques (iris, mtcars, USArrests, swiss, state.x77, airquality)
    \item \textbf{Génération de données synthétiques} : Création de jeux de données artificiels avec paramètres contrôlables (nombre d'observations, bruit, graine aléatoire)
    \item \textbf{Traitement des valeurs manquantes} : Trois stratégies (suppression, imputation par médiane/mode, imputation par moyenne)
    \item \textbf{Prévisualisation interactive} : Affichage tabulaire avec \texttt{DT::datatable}
\end{itemize}

\subsubsection{Configuration et Clustering}

L'onglet "Configuration" propose :
\begin{itemize}
    \item \textbf{Sélection d'algorithme} : Choix entre VAR\_CAH, VAR\_KMEANS, et TandemVarClust avec descriptions contextuelles
    \item \textbf{Sélection de variables} : Interface interactive pour choisir les variables actives
    \item \textbf{Paramétrage algorithmique} : Nombre de clusters ($K$), standardisation, critères d'agrégation (Ward, complete, average, single)
    \item \textbf{Paramètres spécifiques} : Nombre d'initialisations pour VAR\_KMEANS, nombre de bins pour TandemVarClust
    \item \textbf{Détection automatique} : Option de sélection automatique du nombre de clusters
\end{itemize}

L'onglet "Clustering" lance l'analyse avec barre de progression et gestion d'erreurs robuste.

\subsubsection{Visualisation des Résultats}

L'onglet "Résultats" offre une analyse multi-facettes :
\begin{itemize}
    \item \textbf{Résumé textuel} : Statistiques globales (nombre de clusters, distribution des variables, métriques de qualité)
    \item \textbf{Dendrogramme interactif} : Visualisation hiérarchique avec \texttt{plotly} pour VAR\_CAH et TandemVarClust
    \item \textbf{Graphique de Silhouette} : Évaluation de la qualité du clustering par variable
    \item \textbf{Plans factoriels} : Projection 2D des variables dans l'espace factoriel pour TandemVarClust
    \item \textbf{Matrice de corrélation} : Heatmap interactive réorganisée selon les clusters
    \item \textbf{Tables détaillées} : Assignation cluster par cluster avec métriques individuelles
\end{itemize}

L'onglet "Diagnostics" fournit :
\begin{itemize}
    \item \textbf{Métriques globales} : Silhouette moyen, Davies-Bouldin, Dunn Index, Calinski-Harabasz
    \item \textbf{Analyse de stabilité Bootstrap} : Évaluation de la robustesse du clustering par rééchantillonnage
    \item \textbf{Variables discriminantes} : Identification des variables les plus contributives à la structure
\end{itemize}

\subsubsection{Prédiction de Nouvelles Variables}

L'onglet "Prédiction" implémente :
\begin{itemize}
    \item \textbf{Import de variables illustratives} : Chargement de nouvelles variables à classer
    \item \textbf{Génération de variables test} : Création de variables aléatoires pour démonstration
    \item \textbf{Assignation automatique} : Application de la méthode \texttt{predict()} de l'objet R6
    \item \textbf{Résultats enrichis} : 
    \begin{itemize}
        \item Pour VAR\_CAH et VAR\_KMEANS : Scores de corrélation par cluster
        \item Pour TandemVarClust : Table de contingence, test du Chi², V de Cramér, scores de Dice
    \end{itemize}
    \item \textbf{Visualisation comparative} : Positionnement des nouvelles variables dans l'espace des clusters existants
\end{itemize}

\subsubsection{Historique et Sauvegarde}

L'onglet "Historique" permet :
\begin{itemize}
    \item \textbf{Sauvegarde de sessions} : Enregistrement automatique de chaque analyse avec métadonnées (timestamp, algorithme, paramètres, métriques)
    \item \textbf{Gestion multi-sessions} : Conservation de plusieurs analyses pour comparaison
    \item \textbf{Annotations} : Ajout de notes descriptives pour chaque session
    \item \textbf{Rechargement} : Restauration complète d'une session antérieure (données, modèle, configuration)
    \item \textbf{Export/Import} : Sauvegarde des sessions au format RDS pour partage ou archivage
\end{itemize}

\subsubsection{Export de Résultats}

L'onglet "Export" offre :
\begin{itemize}
    \item \textbf{Export tabulaire} : Tables de résultats au format CSV ou Excel
    \item \textbf{Export graphique} : Dendrogrammes et plans factoriels en PNG ou PDF haute résolution
    \item \textbf{Rapport HTML} : Génération automatique d'un rapport complet avec tous les résultats et visualisations
    \item \textbf{Export du modèle} : Sauvegarde de l'objet R6 complet pour réutilisation programmatique
\end{itemize}

\subsection{Aspects Techniques}

\subsubsection{Gestion de l'État Réactif}

L'application utilise \texttt{reactiveValues} pour gérer l'état global :
\begin{itemize}
    \item \texttt{rv\$data} : Données brutes importées
    \item \texttt{rv\$data\_cleaned} : Données après prétraitement
    \item \texttt{rv\$model\_r6} : Objet R6 complet du modèle
    \item \texttt{rv\$model} : Structure adaptée pour Shiny
    \item \texttt{rv\$sessions\_history} : Liste des sessions sauvegardées
    \item \texttt{rv\$prediction\_results} : Résultats de prédiction
\end{itemize}

\subsubsection{Adaptateur R6-Shiny}

Une fonction \texttt{model\_to\_shiny()} convertit les objets R6 en structures compatibles avec l'interface Shiny, extrayant automatiquement les propriétés actives et calculant les métriques de qualité via \texttt{.compute\_metrics()}.

\subsubsection{Robustesse et UX}

L'application intègre :
\begin{itemize}
    \item \textbf{Gestion d'erreurs} : Blocs \texttt{tryCatch} avec notifications contextuelles
    \item \textbf{Validation d'entrées} : Vérification des types de variables selon l'algorithme
    \item \textbf{Barres de progression} : Feedback visuel durant les calculs longs
    \item \textbf{Aide contextuelle} : Onglet "Aide" avec guide pas-à-pas et documentation des métriques
\end{itemize}

\section{Principes SOLID et Bonnes Pratiques}

\subsection{Single Responsibility Principle}

Chaque classe possède une responsabilité unique : \texttt{ClusteringFactory} pour la création, \texttt{ClusteringComparator} pour la comparaison, \texttt{ClusteringEvaluator} pour la sélection de $k$, et \texttt{ClusteringHelper} pour la génération de rapports.

\subsection{Open/Closed Principle}

L'architecture est ouverte à l'extension mais fermée à la modification. L'ajout d'un nouvel algorithme nécessite uniquement la création d'une classe héritant de \texttt{ClusterAnalysis} et l'ajout d'une méthode factory.

\subsection{Dependency Inversion Principle}

Les classes de haut niveau dépendent d'abstractions (interface \texttt{ClusterAnalysis}) plutôt que d'implémentations concrètes, garantissant la flexibilité architecturale.

\section{Applications et Perspectives}

\subsection{Cas d'Usage}

Le package trouve des applications en génomique (regroupement de gènes avec profils d'expression corrélés), analyse sensorielle (synthèse de descripteurs en dimensions perceptives), et marketing (segmentation basée sur données socio-démographiques mixtes avec TandemVarClust).

\subsection{Extensions Envisagées}

Les extensions futures incluent le support natif des variables ordinales, le clustering flou avec assignation probabiliste, la sélection automatique de $k$ via Gap statistic, et l'optimisation par parallélisation avec \texttt{future} et implémentation de boucles critiques en C++ avec Rcpp.

\section{Conclusion}

Le package \texttt{RollerClustR} propose une architecture logicielle cohérente et extensible pour le clustering de variables en R. L'implémentation de trois algorithmes complémentaires (VAR\_CAH, VAR\_KMEANS, TandemVarClust) offre une couverture méthodologique large, adaptée à divers contextes (variables numériques, mixtes, haute dimension).

Les principes de conception appliqués garantissent maintenabilité, extensibilité et reproductibilité. Les innovations méthodologiques, notamment la projection AFDM pour TandemVarClust et les méthodes de prédiction duales, enrichissent les implémentations de référence. L'interface Shiny développée rend ces méthodes accessibles aux utilisateurs non-programmeurs tout en préservant la puissance analytique du package.

La documentation technique exhaustive dans \texttt{/doc}, combinée à l'interface interactive, positionne \texttt{RollerClustR} comme un outil complet pour l'analyse de structure de variables dans des données complexes.

\begin{thebibliography}{99}

\bibitem{chavent2012}
Chavent, M., Kuentz-Simonet, V., Liquet, B., \& Saracco, J. (2012). ClustOfVar: An R Package for the Clustering of Variables. \textit{Journal of Statistical Software}, 50(13), 1-16.

\bibitem{vigneau2003}
Vigneau, E., \& Qannari, E. M. (2003). Clustering of variables around latent components. \textit{Communications in Statistics-Simulation and Computation}, 32(4), 1131-1150.

\bibitem{ward1963}
Ward, J. H., Jr. (1963). Hierarchical grouping to optimize an objective function. \textit{Journal of the American Statistical Association}, 58(301), 236-244.

\bibitem{escofier1979}
Escofier, B. (1979). Traitement simultané de variables quantitatives et qualitatives. \textit{Les Cahiers de l'Analyse des Données}, IV(2), 137-146.

\bibitem{pages2004}
Pagès, J. (2004). Analyse factorielle de données mixtes. \textit{Revue de Statistique Appliquée}, 52(4), 93-111.

\bibitem{dice1945}
Dice, L. R. (1945). Measures of the amount of ecologic association between species. \textit{Ecology}, 26(3), 297-302.

\bibitem{gamma1994}
Gamma, E., Helm, R., Johnson, R., \& Vlissides, J. (1994). \textit{Design Patterns: Elements of Reusable Object-Oriented Software}. Addison-Wesley.

\bibitem{wickham2015}
Wickham, H. (2015). \textit{R Packages: Organize, Test, Document, and Share Your Code}. O'Reilly Media.

\end{thebibliography}

\end{document}
